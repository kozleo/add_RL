{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1f35c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4f27e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocaineWorld(gym.Env):\n",
    "    \n",
    "\n",
    "    def __init__(self, actions=40, trial_length = 60, lever_light_on = 5):\n",
    "        \n",
    "        \n",
    "        self.time = 0 # time step within a single trial\n",
    "        self.trial_length = trial_length # length of trial\n",
    "        self.lever_light_on = lever_light_on # how long the lever light is on for \n",
    "        \n",
    "        self.k = 20 # height of drug dopamine\n",
    "        self.tau = 1/6 # timescale of drug dopamine\n",
    "        self.time_delay = lever_light_on + 5 #delay of drug dopamine\n",
    "        self.sigma = 1 # width of drug dopamine for gaussian response\n",
    "        self.release_type = 'quick_rise'\n",
    "        self.drug_release = 0\n",
    "        \n",
    "        self.observation_space = Box(0, 1, shape=(2,))\n",
    "\n",
    "        # We have actions, corresponding to \"press cocaine lever\", \"press sugar lever\", \"scratch butt\", etc\n",
    "        self.action_space = Discrete(actions)\n",
    "        \n",
    "        self.rewards = np.random.normal(0,1, self.action_space.n)  \n",
    "        self.rewards[0] = 0 # immediate drug reward\n",
    "        self.rewards[1] = 0.1 # immediate sugar reward\n",
    "        \n",
    "        self.best_non_drug_action = np.argmax(self.rewards)\n",
    "        \n",
    "    def reset(self, seed = 42, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        self.time = 0\n",
    "        self.obs = [1, 0]\n",
    "        self.drug_release = 0\n",
    "        \n",
    "        return self.obs\n",
    "    \n",
    "    def _dopamine_release(self):\n",
    "        if self.release_type == 'quick_rise':\n",
    "            return np.maximum(0,self.k*self.tau*(self.time-self.time_delay)*np.exp(-self.tau*(self.time-self.time_delay)))\n",
    "        if self.release_type == 'gauss':\n",
    "            return np.maximum(0,self.k*np.exp(-(self.tau*(self.time-self.time_delay)/sigma)**2))         \n",
    "    \n",
    "\n",
    "    def step(self, action):        \n",
    "        \n",
    "        terminated = self.time >= self.trial_length #check to make sure trial is not done\n",
    "        \n",
    "        reward = self.rewards[action] #compute reward\n",
    "        \n",
    "        self.time += 1 # go foward one timestep in trial   \n",
    "        \n",
    "        if self.time >= self.lever_light_on:\n",
    "            self.obs =  [0 , 1] \n",
    "            \n",
    "            \n",
    "        if (self.time <= self.lever_light_on) and (action == 0):\n",
    "            self.drug_release = 1\n",
    "            \n",
    "        d = self._dopamine_release()\n",
    "        reward += self.drug_release * d\n",
    "        \n",
    "        \n",
    "        return self.obs, reward, self.drug_release * d, terminated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5198033",
   "metadata": {},
   "source": [
    "# Train MLP on addiction task (very simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "57d9a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "    # Build a feedforward neural network.\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def train(hidden_sizes=[32], lr = 1e-2, \n",
    "          epochs=50, batch_size=5000):\n",
    "\n",
    "    # make environment, check spaces, get obs / act dims\n",
    "    env = CocaineWorld()\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_acts = env.action_space.n\n",
    "\n",
    "    # make core of policy network\n",
    "    logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n",
    "\n",
    "    # make function to compute action distribution\n",
    "    def get_policy(obs):\n",
    "        logits = logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    # make action selection function (outputs int actions, sampled from policy)\n",
    "    def get_action(obs):\n",
    "        return get_policy(obs).sample().item()\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    def compute_loss(obs, act, weights):\n",
    "        logp = get_policy(obs).log_prob(act)\n",
    "        return -(logp * weights).mean()\n",
    "\n",
    "    # make optimizer\n",
    "    optimizer = Adam(logits_net.parameters(), lr=lr)\n",
    "\n",
    "    # for training policy\n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_drug = []\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()       # first obs comes from starting distribution\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "        ep_drug = []            # list for drug accrued throughout ep\n",
    "\n",
    "        # render first episode of each epoch\n",
    "        finished_rendering_this_epoch = False\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:                           \n",
    "\n",
    "            # save obs\n",
    "            batch_obs.append(obs)\n",
    "\n",
    "            # act in the environment\n",
    "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            obs, rew, drug, done = env.step(act)\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "            ep_drug.append(drug)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                \n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                batch_weights += [ep_ret] * ep_len\n",
    "                \n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews, ep_drug = env.reset(), False, [], []\n",
    "                \n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > batch_size:\n",
    "                    break\n",
    "\n",
    "                    \n",
    "        batch_obs = np.stack(batch_obs)       \n",
    "        batch_acts = np.stack(batch_acts) \n",
    "        batch_weights = np.stack(batch_weights)\n",
    "        \n",
    "        #print(batch_acts[0:5])\n",
    "        \n",
    "       \n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                                  )\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        return batch_loss, batch_rets, batch_acts\n",
    "    \n",
    "    rewards_over_epochs = []\n",
    "    acts_over_epochs = []\n",
    "\n",
    "    # training loop\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_acts = train_one_epoch()\n",
    "        \n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_rets)))\n",
    "\n",
    "        rewards_over_epochs.append(np.mean(batch_rets))\n",
    "        acts_over_epochs.append(np.mean(batch_acts))\n",
    "\n",
    "    return rewards_over_epochs,acts_over_epochs,env,logits_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "433c5eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 120.744 \t return: 33.199\n",
      "epoch:   1 \t loss: 142.668 \t return: 39.164\n",
      "epoch:   2 \t loss: 202.247 \t return: 55.539\n",
      "epoch:   3 \t loss: 120.550 \t return: 33.320\n",
      "epoch:   4 \t loss: 152.444 \t return: 42.450\n",
      "epoch:   5 \t loss: 204.956 \t return: 57.314\n",
      "epoch:   6 \t loss: 314.369 \t return: 88.080\n",
      "epoch:   7 \t loss: 188.144 \t return: 53.038\n",
      "epoch:   8 \t loss: 268.838 \t return: 76.537\n",
      "epoch:   9 \t loss: 214.199 \t return: 60.981\n",
      "epoch:  10 \t loss: 227.568 \t return: 65.529\n",
      "epoch:  11 \t loss: 341.744 \t return: 99.376\n",
      "epoch:  12 \t loss: 377.744 \t return: 111.446\n",
      "epoch:  13 \t loss: 263.417 \t return: 77.343\n",
      "epoch:  14 \t loss: 305.030 \t return: 91.508\n",
      "epoch:  15 \t loss: 402.968 \t return: 122.601\n",
      "epoch:  16 \t loss: 448.173 \t return: 137.306\n",
      "epoch:  17 \t loss: 399.413 \t return: 123.112\n",
      "epoch:  18 \t loss: 393.891 \t return: 120.992\n",
      "epoch:  19 \t loss: 458.267 \t return: 144.255\n",
      "epoch:  20 \t loss: 498.114 \t return: 156.994\n",
      "epoch:  21 \t loss: 476.777 \t return: 150.420\n",
      "epoch:  22 \t loss: 473.322 \t return: 151.911\n",
      "epoch:  23 \t loss: 465.539 \t return: 149.914\n",
      "epoch:  24 \t loss: 474.842 \t return: 151.421\n",
      "epoch:  25 \t loss: 515.623 \t return: 166.137\n",
      "epoch:  26 \t loss: 512.697 \t return: 167.959\n",
      "epoch:  27 \t loss: 509.958 \t return: 168.164\n",
      "epoch:  28 \t loss: 511.177 \t return: 169.337\n",
      "epoch:  29 \t loss: 492.846 \t return: 161.386\n",
      "epoch:  30 \t loss: 508.243 \t return: 170.807\n",
      "epoch:  31 \t loss: 502.210 \t return: 173.836\n",
      "epoch:  32 \t loss: 508.678 \t return: 171.149\n",
      "epoch:  33 \t loss: 508.943 \t return: 175.617\n",
      "epoch:  34 \t loss: 503.604 \t return: 174.806\n",
      "epoch:  35 \t loss: 488.323 \t return: 171.775\n",
      "epoch:  36 \t loss: 487.257 \t return: 171.325\n",
      "epoch:  37 \t loss: 488.401 \t return: 176.148\n",
      "epoch:  38 \t loss: 505.794 \t return: 182.985\n",
      "epoch:  39 \t loss: 497.949 \t return: 186.097\n",
      "epoch:  40 \t loss: 494.979 \t return: 186.437\n",
      "epoch:  41 \t loss: 486.787 \t return: 189.991\n",
      "epoch:  42 \t loss: 481.189 \t return: 192.533\n",
      "epoch:  43 \t loss: 480.068 \t return: 187.553\n",
      "epoch:  44 \t loss: 474.671 \t return: 194.788\n",
      "epoch:  45 \t loss: 425.617 \t return: 183.661\n",
      "epoch:  46 \t loss: 433.694 \t return: 183.734\n",
      "epoch:  47 \t loss: 436.873 \t return: 193.363\n",
      "epoch:  48 \t loss: 425.640 \t return: 202.714\n",
      "epoch:  49 \t loss: 435.790 \t return: 202.650\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 1e-2\n",
    "\n",
    "\n",
    "rewards_over_epochs,acts_over_epochs,env,logits_net = train(lr=lr,epochs=50, batch_size=1000)\n",
    "\n",
    "#plt.plot(rewards_over_epochs)\n",
    "#print(acts_over_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "989f4096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7210221793635487"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.asarray(env.ds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:leokoz8-add_RL] *",
   "language": "python",
   "name": "conda-env-leokoz8-add_RL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
